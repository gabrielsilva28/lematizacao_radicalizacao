{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer #radicalizar texto\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist #ajuda no calculo da frequencia\n",
    "import re\n",
    "import string\n",
    "from collections import Counter #contador\n",
    "nltk.download('punkt') #ajuda a dividir um texto em uma lista de frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizar_arquivo(arq):\n",
    "    #ler arquivo txt\n",
    "    with open(arq, 'r', encoding='utf-8') as arquivo:\n",
    "        texto = arquivo.read().replace(\"\\uFEFF\", \"\")\n",
    "    #retorna o token\n",
    "    tokens = word_tokenize(texto)\n",
    "    return tokens\n",
    "\n",
    "arquivo = 'The-Iliad-of-Homer.txt'\n",
    "tokenizado = tokenizar_arquivo(arquivo)\n",
    "\n",
    "print(tokenizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funções para limpar o texto\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") #especificar idioma\n",
    "\n",
    "#carrega arquivo text com stopwords\n",
    "def carregar_stopwords(nome_arquivo):\n",
    "    with open(nome_arquivo, 'r', encoding='utf-8') as arquivo:\n",
    "        stopwords = set([linha.strip() for linha in arquivo])\n",
    "    return stopwords\n",
    "\n",
    "def lower_case(text):\n",
    "    # Converte todos os caracteres para minusculo\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Remove pontuacao e outros caracteres especiais usando regex\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    # Remove as stopwords \n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word not in stopwords]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text \n",
    "\n",
    "def processar_txt(nome_arquivo, nome_arquivo_stopwords, language='english'):\n",
    "    # Le o texto do arquivo\n",
    "    with open(nome_arquivo, 'r', encoding='utf-8') as arquivo:\n",
    "        texto = arquivo.read()\n",
    "        \n",
    "    # Carrega as stopwords do arquivo\n",
    "    stopwords = carregar_stopwords(nome_arquivo_stopwords)\n",
    "    # Aplicacao das etapas de normalizacao e limpeza\n",
    "    texto = lower_case(texto)\n",
    "    texto = remove_punctuation(texto)\n",
    "    texto = remove_stopwords(texto, stopwords)\n",
    "    texto = remove_numbers(texto)\n",
    "\n",
    "    return texto\n",
    "\n",
    "nome_arquivo = 'The-Iliad-of-Homer.txt'  \n",
    "arquivo_stopwords = 'stoplist-ingles.txt' \n",
    "texto_processado = processar_txt(nome_arquivo, arquivo_stopwords)\n",
    "print(texto_processado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(texto_processado)\n",
    "tokens = word_tokenize(texto_processado)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar uma lista para armazenar os lemas\n",
    "lemmas = []\n",
    "\n",
    "# Aplicar lematização para cada palavra na lista de tokens\n",
    "for token in tokens:\n",
    "    doc = nlp(token)\n",
    "    for tok in doc:\n",
    "        lemmas.append(tok.lemma_)\n",
    "\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um objeto Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Inicializar uma lista para armazenar os stems\n",
    "stems = []\n",
    "\n",
    "# Aplicar stemming para cada token no texto\n",
    "for token in tokens:\n",
    "    docs = nlp(token)\n",
    "    for tok in docs:\n",
    "        stem = stemmer.stem(tok.text)\n",
    "        stems.append(stem)\n",
    "\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar a frequência dos tokens\n",
    "lema_counts = Counter(lemmas)\n",
    "\n",
    "# Obter os 20 mais frequentes\n",
    "top20_lema = lema_counts.most_common(20)\n",
    "\n",
    "print(top20_lema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar a frequência dos tokens\n",
    "raiz_counts = Counter(stems)\n",
    "\n",
    "# Obter os 20 mais frequentes\n",
    "top20_raiz = lema_counts.most_common(20)\n",
    "\n",
    "print(top20_raiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Funcao distancia_Levenshtein(X, Y, m, n):\n",
    "    Se m igual a 0:\n",
    "        Retorna n\n",
    "    \n",
    "    Se n igual a 0:\n",
    "        Retorna m\n",
    "    \n",
    "    Se X[m-1] igual a Y[n-1]:\n",
    "        custo recebe 0\n",
    "    Senão:\n",
    "        custo recebe 1\n",
    "    \n",
    "    opcao1 recebe distancia_Levenshtein(X, Y, m-1, n) + 1  # Deletar\n",
    "    opcao2 recebe distancia_Levenshtein(X, Y, m, n-1) + 1  # Inserir\n",
    "    opcao3 recebe distancia_Levenshtein(X, Y, m-1, n-1) + custo  # Substituir\n",
    "\n",
    "    \n",
    "    Retorna mínimo(opcao1, opcao2, opcao3)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
